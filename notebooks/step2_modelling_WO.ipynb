{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from maker_nlp.vizualisation import plot_top_k_words_per_sentiment_tfidf, plot_top_k_explicative_words_per_sentiment\n",
    "from maker_nlp.preprocessing import remove_stop_words, convert_to_lowercase, remove_accents, \\\n",
    "    remove_punctuation_and_digits, normalize_text, lemmatize, clean_text\n",
    "\n",
    "from maker_nlp.config import CLASS_DICT, CLASS_NAMES\n",
    "\n",
    "DATA_FOLDER = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_FOLDER / 'final_dataset.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase, sentiment = df.Phrase, df.Sentiment\n",
    "print(f'Shape of Phrase = {phrase.shape}, Shape of Sentiment = {sentiment.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_phrase = phrase.apply(clean_text)\n",
    "cleaned_phrase = cleaned_phrase.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_phrase[sentiment == 0][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_master, X_test_master, y_train, y_test = train_test_split(cleaned_phrase, sentiment[cleaned_phrase.index], \n",
    "                                                                  test_size= 0.2, random_state=42)\n",
    "X_train_master.shape, X_test_master.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing & Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Text Data with Tf-Idf\n",
    "\n",
    "Tf-Idf stand for Term Frequency - Inverse document frequency: these are two methods combined in order to vectorize texts.  \n",
    "\n",
    "This idea came to correct the incapacity of bag-of-words to take into account the importance of words (for example, *the* should always be ignored whereas *car* might be a valuable information). Hence, Tf-Idf balances the imortance of a word ***locally*** (is this word repeated many times in this document?) and ***globally*** (how much is this word repeated in every document?).  \n",
    "If a word is very present in a document but also in all the other documents (the case of *the* for instance), then its tf-idf score will be low. On the contrary, if a word is present in some documents and seems to be important in them, then its score will be high. All the math in tf-idf is to quantify these *seems to be important* and *very present*. Let's give a quick look at it:  \n",
    "\n",
    "$$TfIdf(t,d) = tf(t,d)  \\times  idf(t)$$\n",
    "\n",
    "with t as the term, d the document and:\n",
    "\n",
    "$$tf(t, d) = \\frac{n_{t,d}}{\\sum \\limits _{k} n_{k, d}} $$\n",
    "\n",
    "$$idf(t) = log \\frac{|D|}{|\\{d_{j} : t_{i} \\in d_{j}\\} |}$$\n",
    "\n",
    "So, what we understand from these formulas is that the tf-idf score is a comination of these two factors:\n",
    "&emsp;- The ***term frequency (tf)*** score which is the frequency of the word normalized by the number of words in the document\n",
    "&emsp;- The ***inverse document frequency (idf)*** score which represents the number of documents in the corpus divided by the number of documents where the word appears. The log function serves as a catalyst: the higher the number of documents where the term appears, the lower the idf score.\n",
    "\n",
    "**In conclusion, to have a high tf-idf score, a word has to have a high idf score, which means that it shouldn't appear in too many documents. And for the documents where it appears, it should be repeated a lot!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorized_phrase = vectorizer.fit_transform(X_train_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer().fit(cleaned_phrase)\n",
    "\n",
    "negative_df = vectorizer.transform(X_train_master[y_train == 0])\n",
    "\n",
    "for sentence in negative_df:\n",
    "    sorted_sentence = sorted(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer().fit(cleaned_phrase)\n",
    "\n",
    "#negative_df = vectorizer.transform(cleaned_phrase[sentiment == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inverse_dict = {val: key for key, val in vectorizer.vocabulary_.items()}\n",
    "\n",
    "row = negative_df.getrow(1).toarray()[0].ravel()\n",
    "\n",
    "top_10_indices = row.argsort()[-12:]\n",
    "top_10_values = row[top_10_indices]\n",
    "top_10_words = [inverse_dict[k] for k in top_10_indices]\n",
    "top_10_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(row[row > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_master[y_train == 0].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_k_words_per_sentiment_tfidf(X_train_master, y_train, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Negative\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_neg = TfidfVectorizer()\n",
    "count_neg = CountVectorizer()\n",
    "\n",
    "vect_neg.fit(X_train_master[y_train == CLASS_DICT['negative']])\n",
    "counts = count_neg.fit_transform(X_train_master[y_train == CLASS_DICT['negative']])\n",
    "counts = counts.sum(axis = 0).reshape((-1, 1))\n",
    "\n",
    "negative_dict = vect_neg.vocabulary_\n",
    "\n",
    "inverse_dict = {val: key for key, val in vectorizer.vocabulary_.items()}\n",
    "\n",
    "idf_df = pd.DataFrame([[inverse_dict[i], x] for i, x in enumerate(vectorizer.idf_)], columns = ['word', 'idf_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df = idf_df.sort_values(by = 'idf_score', ascending = False).reset_index().drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neg_words = []\n",
    "\n",
    "neg_inverse_dict = {val: key for key, val in vect_neg.vocabulary_.items()}\n",
    "counts_df = pd.DataFrame([[neg_inverse_dict[i], x[0,0]] for i, x in enumerate(counts)], columns = ['word', 'count'])\n",
    "counts_df = counts_df[counts_df['count'] > 4]\n",
    "\n",
    "for index, (word, score) in idf_df.iterrows():\n",
    "    if word in counts_df.word.values:\n",
    "        top_neg_words.append([word, score])\n",
    "    if len(top_neg_words) ==20:\n",
    "        break\n",
    "top_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_k_explicative_words_per_sentiment(X_train_master, y_train, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(vectorized_phrase, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_master.copy()\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = plot_confusion_matrix(clf, X_test, y_test,\n",
    "                             display_labels=CLASS_NAMES, cmap=plt.cm.Blues,\n",
    "                             normalize='true', ax=ax)\n",
    "disp.ax_.set_title('Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
