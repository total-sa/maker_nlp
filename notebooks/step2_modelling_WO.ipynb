{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from maker_nlp.vizualisation import plot_top_k_words_per_sentiment_tfidf, plot_top_k_explicative_words_per_sentiment\n",
    "from maker_nlp.preprocessing import remove_stop_words, convert_to_lowercase, remove_accents, \\\n",
    "    remove_punctuation_and_digits, normalize_text, lemmatize, clean_text\n",
    "\n",
    "from maker_nlp.config import CLASS_DICT, CLASS_NAMES\n",
    "\n",
    "DATA_FOLDER = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_FOLDER / 'final_dataset.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase, sentiment = df.Phrase, df.Sentiment\n",
    "print(f'Shape of Phrase = {phrase.shape}, Shape of Sentiment = {sentiment.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_phrase = phrase.apply(clean_text)\n",
    "cleaned_phrase = cleaned_phrase.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_phrase[sentiment == 0][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_master, X_test_master, y_train, y_test = train_test_split(cleaned_phrase, sentiment[cleaned_phrase.index], \n",
    "                                                                  test_size= 0.2, random_state=42)\n",
    "X_train_master.shape, X_test_master.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing & Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Text Data with Tf-Idf\n",
    "\n",
    "Tf-Idf stand for Term Frequency - Inverse document frequency: these are two methods combined in order to vectorize texts.  \n",
    "\n",
    "This idea came to correct the incapacity of bag-of-words to take into account the importance of words (for example, *the* should always be ignored whereas *car* might be a valuable information). Hence, Tf-Idf balances the imortance of a word ***locally*** (is this word repeated many times in this document?) and ***globally*** (how much is this word repeated in every document?).  \n",
    "If a word is very present in a document but also in all the other documents (the case of *the* for instance), then its tf-idf score will be low. On the contrary, if a word is present in some documents and seems to be important in them, then its score will be high. All the math in tf-idf is to quantify these *seems to be important* and *very present*. Let's give a quick look at it:  \n",
    "\n",
    "$$TfIdf(t,d) = tf(t,d)  \\times  idf(t)$$\n",
    "\n",
    "with t as the term, d the document and:\n",
    "\n",
    "$$tf(t, d) = \\frac{n_{t,d}}{\\sum \\limits _{k} n_{k, d}} $$\n",
    "\n",
    "$$idf(t) = log \\frac{|D|}{|\\{d_{j} : t_{i} \\in d_{j}\\} |}$$\n",
    "\n",
    "So, what we understand from these formulas is that the tf-idf score is a comination of these two factors:\n",
    "&emsp;- The ***term frequency (tf)*** score which is the frequency of the word normalized by the number of words in the document\n",
    "&emsp;- The ***inverse document frequency (idf)*** score which represents the number of documents in the corpus divided by the number of documents where the word appears. The log function serves as a catalyst: the higher the number of documents where the term appears, the lower the idf score.\n",
    "\n",
    "**In conclusion, to have a high tf-idf score, a word has to have a high idf score, which means that it shouldn't appear in too many documents. And for the documents where it appears, it should be repeated a lot!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorized_phrase = vectorizer.fit_transform(X_train_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_k_explicative_words_per_sentiment(X_train_master, y_train, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(vectorized_phrase, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_master.copy()\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = plot_confusion_matrix(clf, X_test, y_test,\n",
    "                             display_labels=CLASS_NAMES, cmap=plt.cm.Blues,\n",
    "                             normalize='true', ax=ax)\n",
    "disp.ax_.set_title('Normalized confusion matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
